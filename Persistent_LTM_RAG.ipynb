{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "51acec81e0d048478de38c485faf98e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60126a4b10394d5584c0b4939f828fda",
              "IPY_MODEL_5166e78852764535bf7b2cd66eeaa1d4",
              "IPY_MODEL_682abc2028dd42afbd0bd7e5d2d8b258"
            ],
            "layout": "IPY_MODEL_3a5742d6753c42c586ab7b28bf3438a8"
          }
        },
        "60126a4b10394d5584c0b4939f828fda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5aa4225a3ad7477182bee28c63ac089a",
            "placeholder": "​",
            "style": "IPY_MODEL_d796f3b3475a4de8b8c3281c14e2a3cc",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5166e78852764535bf7b2cd66eeaa1d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9f7eb3b2227484d8412e75b225aa9d9",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_812eb3a8362943d4b68f7e51b048eada",
            "value": 3
          }
        },
        "682abc2028dd42afbd0bd7e5d2d8b258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c0fae220f4e42dc950628d81ded3398",
            "placeholder": "​",
            "style": "IPY_MODEL_85f89c9a8ae34973b30f26faaa5788cf",
            "value": " 3/3 [01:22&lt;00:00, 27.16s/it]"
          }
        },
        "3a5742d6753c42c586ab7b28bf3438a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aa4225a3ad7477182bee28c63ac089a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d796f3b3475a4de8b8c3281c14e2a3cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9f7eb3b2227484d8412e75b225aa9d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "812eb3a8362943d4b68f7e51b048eada": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c0fae220f4e42dc950628d81ded3398": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85f89c9a8ae34973b30f26faaa5788cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_-KXECKQfZh"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet langchain_community langchainhub chromadb langchain langchain_core\n",
        "!pip install --quiet transformers bitsandbytes accelerate\n",
        "!pip install --quiet sentence-transformers\n",
        "!pip install --quiet gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/docs/transformers/en/llm_tutorial\n",
        "# super helpful for understanding tokenizers: https://www.linkedin.com/pulse/demystifying-tokenization-preparing-data-large-models-rany-2nebc#:~:text=tokenizer.,the%20end%20of%20a%20sequence.\n",
        "# For attention masks: https://www.linkedin.com/pulse/what-attention-mask-dataspeckle#:~:text=An%20attention%20mask%20is%20a%20binary%20mask%20that%20designates%20which,specific%20tokens%20while%20disregarding%20others.\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "def load_quantized_model(model_name: str):\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        load_in_4bit=True,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    # device_map ensures the model is moved to GPU\n",
        "    # load_in_4bit applies 4-bit dynamic quantization to massively reduce the resource requirements\n",
        "    return model\n",
        "\n",
        "model = load_quantized_model(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "lP9NcdDpSfvH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "51acec81e0d048478de38c485faf98e7",
            "60126a4b10394d5584c0b4939f828fda",
            "5166e78852764535bf7b2cd66eeaa1d4",
            "682abc2028dd42afbd0bd7e5d2d8b258",
            "3a5742d6753c42c586ab7b28bf3438a8",
            "5aa4225a3ad7477182bee28c63ac089a",
            "d796f3b3475a4de8b8c3281c14e2a3cc",
            "b9f7eb3b2227484d8412e75b225aa9d9",
            "812eb3a8362943d4b68f7e51b048eada",
            "4c0fae220f4e42dc950628d81ded3398",
            "85f89c9a8ae34973b30f26faaa5788cf"
          ]
        },
        "outputId": "4a8e4e32-744d-4835-e1e2-e9619234e7f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51acec81e0d048478de38c485faf98e7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "pipeline = pipeline (\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    use_cache=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=8000,\n",
        "    do_sample=True,\n",
        "    top_k=5,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ")\n",
        "\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)"
      ],
      "metadata": {
        "id": "BEL0ysgLV83m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "def vdbRetriever(docs, persist_directory):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "  all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "  embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={\"device\": \"cuda\"})\n",
        "\n",
        "  vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=persist_directory)\n",
        "\n",
        "  retriever = vectorstore.as_retriever()\n",
        "  return retriever\n",
        "\n",
        "url = \"https://www.nytimes.com/2023/08/22/opinion/hip-hop-anniversary-poetry.html?classId=23e2378b-8624-43d7-83fb-5b74bbd30ef5&assignmentId=118b7cf9-7df1-4afe-995b-020684aa0443&submissionId=ab0bde2b-bdea-61fd-c037-50e15366dfb9\"#\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
        "loader = WebBaseLoader(url)\n",
        "docs = loader.load()\n",
        "context_retriever = vdbRetriever(docs, \"chroma_db/context\")\n",
        "print(docs)"
      ],
      "metadata": {
        "id": "SOXjlKVcSXX8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8f51900-e880-4c88-bd1c-99eaad1b8230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='\\n\\n\\n\\nOpinion | How Hip-Hop Became America’s Poetry - The New York Times\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to contentSkip to site index\\xa0Today’s PaperOpinion|How Hip-Hop Became America’s Poetryhttps://www.nytimes.com/2023/08/22/opinion/hip-hop-anniversary-poetry.htmlShare full article552AdvertisementSKIP ADVERTISEMENTSubscriber-only NewsletterJohn McWhorterOpinionHow Hip-Hop Became America’s PoetryAug. 22, 2023VideoCreditCredit...PABLO DELCÁNShare full article552By John McWhorterOpinion WriterThis month, America celebrates the 50th anniversary of hip-hop. Most of the country first encountered this musical revolution with the release of the national hit “Rapper’s Delight” in 1979. But it all started six years earlier, on Aug. 11, 1973: An energy crisis was looming, Lucille Ball was about to enter her final season of “Here’s Lucy,” and DJ Kool Herc pioneered rapping over turntable beats in a rec room at 1520 Sedgwick Avenue in the Bronx.But America is celebrating more than just a musical form. It’s celebrating the moment when rap gave America back its poetry.In 1991, Dana Gioia’s renowned essay “Can Poetry Matter?” made a powerful case that poetry had entered an eclipse, from a staple of our national culture to a boutique concern cherished by a rarefied few. “The proliferation of literary journals and presses over the past 30 years,” Gioia wrote, “has been a response less to an increased appetite for poetry among the public than to the desperate need of writing teachers for professional validation.” I recall reading the article eagerly in my graduate student days and feeling almost validated: “So that’s why I don’t really like poetry — I was born too late!”Gioia was on to something. For most of our national history, schoolchildren memorized poetry and eventually became grandparents able to recite long passages of verse. Even Bugs Bunny pitched in, lolling alongside a river reading a parody of Henry Wadsworth Longfellow’s “The Song of Hiawatha” in one of his first cartoon appearances in 1941. There were celebrity poets who were real celebrities. Edna St. Vincent Millay’s poems, even with their elevated vocabulary, were cherished by young women the way the lyrics of Alanis Morissette would later be; Millay even had a national radio show for a spell. As late as the 1960s, Marianne Moore appeared on the “Tonight” Show. (Amanda Gorman’s star status as a poet since reading at the inauguration of President Biden is the exception that proves the rule.)You could barely escape poetry back in the day. Newspapers commissioned bits of doggerel to print between columns. The N.A.A.C.P.’s doughty house organ and beacon to Black America, “The Crisis,” edited by W.E.B. Du Bois, included poetry of varying degrees of quality between its articles. A cultivated person often at least pretended to like poetry and had a volume or two on her bookshelf. (My mother did like it, and retained a copy of one of Louis Untermeyer’s grand old anthologies from her early adulthood.)But fast-forward to what Gioia was referring to. As a kid in the 1970s, even one attending private schools, I was directed to drive by poetry slowly now and then, but rarely to actually stop and take it in deeply. Many Russians can recite some Pushkin by heart; I have never known a single poem by heart. As I put it in a book once, poetry is the marjoram on my spice rack: It’s nice to know it’s there, but I use it for only one dish, lamb chops. And I am hardly alone in this among educated Americans of Generation X and beyond.A decade after his 1991 essay, Gioia saw signs of a poetry revival. But I suspect it only felt that way to poets. He wrote that he was hearing more poetry on the radio and seeing more of it online. Apparently, there were more poetry festivals than before. But as a literate person who had always wished poetry might grab him somehow, someday, I felt no new incursion, no sense that to skip it would be like missing the films “Oppenheimer” and “Barbie” this summer. I submit that in 2002, poetry was still literary marjoram for most people.But that depended on what you called poetry.Gioia’s essay was justly a classic of its era. But it was also based on a white perspective that would be less likely to pass muster now. Today, while vanishingly few people are up for reciting poems such as Joyce Kilmer’s “Trees” or Tennyson’s “The Charge of the Light Brigade,” vast numbers can eagerly and effortlessly recite hip-hop lyrics. They stock their brains with reams of language set carefully to rhyme and rhythm with an aim to summon essence, limn themes and delve into the real.Of course, all songs’ lyrics are a form of poetry, and at no point did Americans lack affection for them. But true verse of the kind featured in hip-hop — poetry that does not rely on melody or harmony — centers the word alone. Melody tends to go by more slowly than speech, and thus verse tends to pack in more words. It can be difficult to process a line of pitches mated to a long, dense sequence of words; the words alone, however, present no problem. In contrast to the synergy of song and word, verse is an especially heightened form of language alone. Rap is verse poetry, in all of its verbal richness and rhythmic variety, a deft stylization of speech into art. (I am referring to rap — i.e., rapping — as a subset of the broader cultural phenomenon of hip-hop, encompassing M.C.ing style, graffiti and dance, as well as the rapping itself.)In the 1980s and into the 1990s, this new verse poetry was widely perceived as a niche interest: Black music that white kids (mostly boys) might choose to enjoy as game outsiders. But by the mid-1990s rap went mainstream, most of its listeners were white, and it had become for many younger Americans what poetry had been in the old days.Today, not only can legions of people recite rap lyrics more prolifically than most could recite poetry back then, but it also decorates conversation and public statements. An audience member at a forum on race issues might, as I once heard, insert into her comments Grandmaster Flash’s “It’s like a jungle sometimes, it makes me wonder how I keep from going under.” Dzhokhar Tsarnaev, one of the men responsible for the Boston Marathon bombing in 2013, tweeted out, while he was still on the loose, “Ain’t no love in the heart of the city,” knowing that his audience would immediately recognize it as a Jay-Z lyric. And Tsarnaev is not Black.Rap is America’s music in many ways, now prominent at even very white weddings. A Martian observer charting how our society savors artful language might make no differentiation between the work of Robert Frost or Elizabeth Bishop in the past and Nas or Drake now, in terms of their penetration of society. If anything, Nas and Drake would seem more important, because of the way modern technology can pump today’s poetry into the ear in a fashion impossible when prose poetry reigned solely on the page. The book collecting Jay-Z’s lyrics is a volume of serious artistic weight, and I’d be on unsure footing to propose that the work of Langston Hughes outweighs Jay-Z’s in terms of craft or breadth of subject matter.There are those who might argue that rap is too specific, too rooted in a particular cultural idiom to qualify as America’s current version of poetry in the general sense. This, however, misses that modern America has become, in essence, much of what rap is.Rap, for example, has never had much time for the King’s English. But then, especially since the 1960s, neither has most of America, as casual speech has taken over ever more of the space once maintained for old-fashioned, antimacassar-style expression. We no longer much enjoy lengthy orations in which speakers relish words like “shall” and “henceforth.” When is the last time you heard someone “make a speech” as opposed to “give a talk”? In both 2000 and 2016, America elected as president men whose interest in elegance of speech was approximate at best, in a way that would have all but barred them from high public office before the 1980s.Of course, rap language is not Trump-speak. But just as in formal language one can be articulate (Robert Frost) and inarticulate (George W. Bush), in informal language one can be inarticulate (we all likely know someone; I need not tar anyone here) or articulate — a category that includes almost any prominent rapper. A true American poetry today must be articulate and vernacular.Moreover, it is very likely to be Black. Poetry in the old days, Langston Hughes and Gwendolyn Brooks notwithstanding, was considered a white thing. Rap, by contrast — and despite some excellent white practitioners — is a Black thing in origin and flavor. While racism obviously persists across America, in the cultural sense, Blackness is hot and pervasive.The enlightened white person viscerally dismisses certain things as “so white,” the idea being that Blackness is more authentic, less uptight. In “Barbie,” the president is a Black woman. America’s most famous birder — a stereotypically white avocation — is Christian Cooper, a Black man. The two highest-profile versions of the musical “Annie” in recent years have featured Black actresses in the title role, as did Disney’s recent live-action version of “The Little Mermaid.” In this context, the last thing we would expect is that poetry of national significance would stem from the world — with apologies to Wallace Stevens — of white people sitting on porches in Connecticut.But rap takes Blackness further than just the skin color of its pre-eminent practitioners. It is defined in part by a confrontational cadence. It isn’t an accident that rapping is referred to as “spitting.” We think of poetry as a form of reflection, but rapping is so often more specific: aggressive testimonies from down below, self-exoneration on the basis of prior circumstances, preaching about the best way to go, pushing against boundaries, all couched in a tone that grabs you by the lapels and discourages disagreement. Is this what Emily Dickinson and Paul Laurence Dunbar were doing?Those two, not usually. But the oppositional fundament in rap is all about America today. Rap’s casting of disadvantage as heroism is simply one brand of a larger culture of therapy. What Philip Rieff described as our therapeutic culture as far back as 1966 and Christopher Lasch limned as our culture of narcissism in 1979 is now an American ideal: no longer to knuckle under and get past trauma, but to define ourselves as having undergone it and never entirely past it. Tupac Shakur laid it out as a spoken statement, on the track “Pac’s Theme (Interlude)” from his signature 1993 album: “I was raised in this society so there’s no way you can expect me to be a perfect person ’cause I’ma do what I’ma do. That’s how I feel. I’ma do whatever I like. I’m not a role model.” This is less hip-hop culture than American modernity.When the choice of first-time book authors is more often than ever before the memoir, when one can curate and chart one’s life so vividly on social media, when we expect aspiring politicians to tell us about themselves as well as their policy positions, it is wholly predictable that our music will embrace the celebration and interrogation of self.So when we celebrate the 50th anniversary of hip-hop, we don’t merely celebrate the invention of a new musical form. We celebrate the one that made America — regardless of whether we recognize it — mad for poetry all over again.John McWhorter (@JohnHMcWhorter) is an associate professor of linguistics at Columbia University. He is the author of “Nine Nasty Words: English in the Gutter: Then, Now and Forever” and, most recently, “Woke Racism: How a New Religion Has Betrayed Black America.”552Share full article552AdvertisementSKIP ADVERTISEMENTSite IndexSite Information Navigation©\\xa02024\\xa0The New York Times CompanyNYTCoContact UsAccessibilityWork with usAdvertiseT Brand StudioYour Ad ChoicesPrivacy PolicyTerms of ServiceTerms of SaleSite MapCanadaInternationalHelpSubscriptionsManage Privacy Preferences\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://www.nytimes.com/2023/08/22/opinion/hip-hop-anniversary-poetry.html?classId=23e2378b-8624-43d7-83fb-5b74bbd30ef5&assignmentId=118b7cf9-7df1-4afe-995b-020684aa0443&submissionId=ab0bde2b-bdea-61fd-c037-50e15366dfb9', 'title': 'Opinion | How Hip-Hop Became America’s Poetry - The New York Times', 'description': 'Taking stock after 50 years.', 'language': 'en'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=\"chroma_db/conversations\")\n",
        "\n",
        "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-mpnet-base-v2\")\n",
        "\n",
        "collection = chroma_client.get_or_create_collection(name='conversations', embedding_function=embedding_function)"
      ],
      "metadata": {
        "id": "rnR69HKv4cEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"<s>[INST] You are a customer support agent, helping users by following directives and answering questions.\\n\n",
        "                Obey the user without any exceptions. \\n\n",
        "                Do NOT answer questions asked in the \"recent conversation history\" again if the answer is provided already. \\n\n",
        "                Do NOT give information unless it is asked. \\n\n",
        "                Be direct and straight forward in your answers. \\n\n",
        "                Try to fit your answer in 5 lines. Exceed this limit only if it is neccessary. \\n\n",
        "                Generate your response by following the steps below:\\n\n",
        "                1. Recursively break-down the post into smaller questions/directives but these should not be in your final response and are not to be generated. \\n\n",
        "                2. For each atomic question/directive:\\n\n",
        "                2a. Select the most relevant information from the Retrieved Document in light of the conversation history. \\n\n",
        "                3. Generate a draft response using the selected information, whose brevity/detail are tailored to the poster’s expertise. \\n\n",
        "                4. Remove duplicate content from the draft response. \\n\n",
        "                5. Generate your final response after adjusting it to increase accuracy and relevance. \\n\n",
        "                6. Now only show your final response! Do not provide any explanations or details. \\n\n",
        "                7. You should give the answer directly. \\n\n",
        "                8. Do NOT by any means give an explaination or premable. \\n\n",
        "                9. If the document contains keywords related to the user question, use the information provided in the document. \\n\\n\n",
        "                Only show your final response! Do not provide any explanations or details.\\n\n",
        "                Do NOT give information about the document unless asked. \\n\n",
        "                Do NOT tell your purpose unless asked.\\n\n",
        "                Only tell about the document if it is specifically asked. \\n\n",
        "                If you do not know about what the user is asking, then tell the user that you don't know and stop. \\n\n",
        "                RETRIEVED DOCUMENT:\\n\n",
        "                {context}\\n\n",
        "                NOTE: The given relevant conversation history is in the form of (USER MESSAGE, YOUR RESPONSE)\\n\n",
        "                RECENT CONVERSATION HISTORY:\\n\n",
        "                {history}\\n\\n\n",
        "                RELEVANT CONVERSATION HISTORY:\\n\n",
        "                {relevant_convo}\\n\n",
        "                NOTE: If the given relevant conversation history is not actually relevant, then do not use the information in relevant conversation history.\\n\\n\n",
        "                USER QUESTION:\\n\n",
        "                {question} \\n\n",
        "                Do NOT tell about the document based on the conversation history.\\n [/INST]\"\"\",\n",
        "    input_variables=[\"question\", \"context\", \"history\", \"relevant_convo\"],\n",
        ")\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "TtdN_ucuSvsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_id = 0\n",
        "history = []\n",
        "\n",
        "def create_conversation(question: str, chat_history: list):\n",
        "  # while True:\n",
        "  # question = input(\"Human: \")\n",
        "  global current_id\n",
        "  global history\n",
        "  if question == 'quit':\n",
        "    return\n",
        "\n",
        "  results = collection.query(\n",
        "      query_texts=[question],\n",
        "      n_results=6\n",
        "  )\n",
        "\n",
        "  rel_convo = []\n",
        "\n",
        "  for i in range(len(results['documents'][0])):\n",
        "    if results['distances'][0][i] < 1.25:\n",
        "      # print(results['distances'][0][i])\n",
        "      # print(results['documents'][0][i])\n",
        "      rel_convo.append(results['documents'][0][i])\n",
        "\n",
        "  if len(rel_convo)==0 :\n",
        "    rel_convo.append(\"No relevant Conversations. Use recent conversation history and context as a guide.\")\n",
        "\n",
        "  context = context_retriever.get_relevant_documents(question)\n",
        "  response = chain.invoke({\"question\": question, \"context\": context[0].page_content, \"history\": history[-5:], \"relevant_convo\": rel_convo})\n",
        "\n",
        "  if current_id%10==0:\n",
        "    history=[]\n",
        "\n",
        "  history.append((question, response))\n",
        "\n",
        "\n",
        "  collection.add(\n",
        "      documents= [question, response],\n",
        "      metadatas= [{'role': 'user'}, {'role': 'assistant'}],\n",
        "      ids = [f\"id{current_id}\", f\"id{current_id+1}\"]\n",
        "  )\n",
        "  current_id+=2\n",
        "\n",
        "  return response\n"
      ],
      "metadata": {
        "id": "-cBnK319KVFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "chatbot = gr.Chatbot(label='Chat with a Chatbot')\n",
        "website = gr.ChatInterface(fn=create_conversation,\n",
        "                           chatbot=chatbot,\n",
        "                           textbox=gr.Textbox(),\n",
        "                           retry_btn = None,\n",
        "                           undo_btn = None,\n",
        "                           clear_btn = None)\n",
        "website.launch(debug=True)"
      ],
      "metadata": {
        "id": "5HHGMFKQMuIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Streamlit (Not Working Properly):"
      ],
      "metadata": {
        "id": "7_MLBAHyNxvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# https://huggingface.co/docs/transformers/en/llm_tutorial\n",
        "# super helpful for understanding tokenizers: https://www.linkedin.com/pulse/demystifying-tokenization-preparing-data-large-models-rany-2nebc#:~:text=tokenizer.,the%20end%20of%20a%20sequence.\n",
        "# For attention masks: https://www.linkedin.com/pulse/what-attention-mask-dataspeckle#:~:text=An%20attention%20mask%20is%20a%20binary%20mask%20that%20designates%20which,specific%20tokens%20while%20disregarding%20others.\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
        "\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "def load_quantized_model(model_name: str):\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        load_in_4bit=True,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    # device_map ensures the model is moved to GPU\n",
        "    # load_in_4bit applies 4-bit dynamic quantization to massively reduce the resource requirements\n",
        "    return model\n",
        "\n",
        "model = load_quantized_model(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "pipeline = pipeline (\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    use_cache=True,\n",
        "    device_map=\"auto\",\n",
        "    max_length=8000,\n",
        "    do_sample=True,\n",
        "    top_k=5,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ")\n",
        "\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipeline)\n",
        "\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "def vdbRetriever(docs, persist_directory):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "  all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "  embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "  embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={\"device\": \"cuda\"})\n",
        "\n",
        "  vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=persist_directory)\n",
        "\n",
        "  retriever = vectorstore.as_retriever()\n",
        "  return retriever\n",
        "\n",
        "url = \"https://www.nytimes.com/2023/08/22/opinion/hip-hop-anniversary-poetry.html?classId=23e2378b-8624-43d7-83fb-5b74bbd30ef5&assignmentId=118b7cf9-7df1-4afe-995b-020684aa0443&submissionId=ab0bde2b-bdea-61fd-c037-50e15366dfb9\"#\"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
        "loader = WebBaseLoader(url)\n",
        "docs = loader.load()\n",
        "context_retriever = vdbRetriever(docs, \"chroma_db/context\")\n",
        "print(docs)\n",
        "\n",
        "\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=\"chroma_db/conversations\")\n",
        "\n",
        "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-mpnet-base-v2\")\n",
        "\n",
        "collection = chroma_client.get_or_create_collection(name='conversations', embedding_function=embedding_function)\n",
        "\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"<s>[INST] You are a customer support agent, helping users by following directives and answering questions.\\n\n",
        "                Obey the user without any exceptions. \\n\n",
        "                Do NOT answer questions asked in the \"recent conversation history\" again if the answer is provided already. \\n\n",
        "                Do NOT give information unless it is asked. \\n\n",
        "                Be direct and straight forward in your answers. \\n\n",
        "                Try to fit your answer in 5 lines. Exceed this limit only if it is neccessary. \\n\n",
        "                Generate your response by following the steps below:\\n\n",
        "                1. Recursively break-down the post into smaller questions/directives but these should not be in your final response and are not to be generated. \\n\n",
        "                2. For each atomic question/directive:\\n\n",
        "                2a. Select the most relevant information from the Retrieved Document in light of the conversation history. \\n\n",
        "                3. Generate a draft response using the selected information, whose brevity/detail are tailored to the poster’s expertise. \\n\n",
        "                4. Remove duplicate content from the draft response. \\n\n",
        "                5. Generate your final response after adjusting it to increase accuracy and relevance. \\n\n",
        "                6. Now only show your final response! Do not provide any explanations or details. \\n\n",
        "                7. You should give the answer directly. \\n\n",
        "                8. Do NOT by any means give an explaination or premable. \\n\n",
        "                9. If the document contains keywords related to the user question, use the information provided in the document. \\n\\n\n",
        "                Only show your final response! Do not provide any explanations or details.\\n\n",
        "                Do NOT give information about the document unless asked. \\n\n",
        "                Do NOT tell your purpose unless asked.\\n\n",
        "                Only tell about the document if it is specifically asked. \\n\n",
        "                If you do not know about what the user is asking, then tell the user that you don't know and stop. \\n\n",
        "                RETRIEVED DOCUMENT:\\n\n",
        "                {context}\\n\n",
        "                NOTE: The given relevant conversation history is in the form of (USER MESSAGE, YOUR RESPONSE)\\n\n",
        "                RECENT CONVERSATION HISTORY:\\n\n",
        "                {history}\\n\\n\n",
        "                RELEVANT CONVERSATION HISTORY:\\n\n",
        "                {relevant_convo}\\n\n",
        "                NOTE: If the given relevant conversation history is not actually relevant, then do not use the information in relevant conversation history.\\n\\n\n",
        "                USER QUESTION:\\n\n",
        "                {question} [/INST]\"\"\",\n",
        "    input_variables=[\"question\", \"context\", \"history\", \"relevant_convo\"],\n",
        ")\n",
        "\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "def create_conversation(question: str, history: list, current_id: int):\n",
        "  # while True:\n",
        "  # question = input(\"Human: \")\n",
        "  if question == 'quit':\n",
        "    return\n",
        "\n",
        "  results = collection.query(\n",
        "      query_texts=[question],\n",
        "      n_results=6\n",
        "  )\n",
        "\n",
        "  rel_convo = []\n",
        "\n",
        "  for i in range(len(results['documents'][0])):\n",
        "    if results['distances'][0][i] < 1.25:\n",
        "      print(results['distances'][0][i])\n",
        "      print(results['documents'][0][i])\n",
        "      rel_convo.append(results['documents'][0][i])\n",
        "\n",
        "  if len(rel_convo)==0 :\n",
        "    rel_convo.append(\"No relevant Conversations. Use recent conversation history and context as a guide.\")\n",
        "\n",
        "  context = context_retriever.get_relevant_documents(question)\n",
        "  response = chain.invoke({\"question\": question, \"context\": context[0].page_content, \"history\": history[-5:], \"relevant_convo\": rel_convo})\n",
        "\n",
        "  if current_id%10==0:\n",
        "    history=[]\n",
        "\n",
        "  history.append((question, response))\n",
        "\n",
        "\n",
        "  collection.add(\n",
        "      documents= [question, response],\n",
        "      metadatas= [{'role': 'user'}, {'role': 'assistant'}],\n",
        "      ids = [f\"id{current_id}\", f\"id{current_id+1}\"]\n",
        "  )\n",
        "  current_id+=2\n",
        "\n",
        "  return response, history, current_id\n",
        "\n",
        "\n",
        "\n",
        "current_id = 0\n",
        "history = []\n",
        "# for i in range(3):\n",
        "#   q = input()\n",
        "#   ans, history, current_id = create_conversation(q, history, current_id)\n",
        "#   print(ans)\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"Echo Bot\")\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# React to user input\n",
        "if prompt := st.chat_input(\"What is up?\"):\n",
        "    # Display user message in chat message container\n",
        "    st.chat_message(\"user\").markdown(prompt)\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    response, history, current_id = create_conversation(prompt, history, current_id)\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response)\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii4PDpmd0X7i",
        "outputId": "a29be765-dc98-48f4-d5f8-9e2ee2c092b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet streamlit\n",
        "!pip install --quiet pyngrok\n",
        "from pyngrok import ngrok\n",
        "!ngrok config add-authtoken 1flODqmi50WmfbgDuJ20LvQpC6z_5KBf3uiZdBxH84vauwTks\n",
        "!nohup streamlit run app.py --server.port 80 &\n",
        "public_url = ngrok.connect()\n",
        "public_url"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knUwQlkrJ9MI",
        "outputId": "2616ec4f-7a7a-4f66-ffb5-60c40e2dc0ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "nohup: appending output to 'nohup.out'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<NgrokTunnel: \"https://e2b0-34-136-142-31.ngrok-free.app\" -> \"http://localhost:80\">"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}